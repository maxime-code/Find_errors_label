# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NQGZQWzwVr421-e5MLXmVw98McH648dm

# install the dependencies
"""

# Install dependencies
!pip install pyyaml==5.1
!pip install 'git+https://github.com/facebookresearch/detectron2.git'

# Import necessary libraries
import torch
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# Import common libraries
import numpy as np
import os, json, cv2, random
from google.colab.patches import cv2_imshow

# Import detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog

import requests
from PIL import Image
from io import BytesIO

import cv2
from google.colab.patches import cv2_imshow
from detectron2.config import get_cfg
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Define the path to your local image
image_path = '/content/drive/MyDrive/dataset/train/img/00abd8a7-ecd6fc56.jpg'

# Step 2: Read the image using OpenCV
im = cv2.imread(image_path)
print(im)
#cv2_imshow(im)  # Display the image

# Step 3: Configure detectron2 for object detection
cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml"))
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.50  # Set threshold for this model
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml")

# Initialize the predictor
predictor = DefaultPredictor(cfg)

# Perform object detection
outputs = predictor(im)

# Classes of interest
classes_of_interest = {
    0: '__background__',
    1: 'person',
    2: 'bicycle',
    3: 'car',
    4: 'motorcycle',
    5: 'airplane',
    6: 'bus',
    7: 'train',
    8: 'truck',
    9: 'boat',
    10: 'traffic light',
    11: 'fire hydrant',
    12: 'stop sign',

}

# Filter the outputs to include only the classes of interest
instances = outputs["instances"].to("cpu")
pred_classes = instances.pred_classes.numpy()
indices = [i for i, cls in enumerate(pred_classes) if cls in classes_of_interest]

# Keep only the instances of the classes of interest
filtered_instances = instances[indices]

# Visualize the results
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog

# Get the metadata for the COCO dataset
metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])

# Create a Visualizer
v = Visualizer(im[:, :, ::-1], metadata, scale=1.2)
out = v.draw_instance_predictions(filtered_instances)

cv2_imshow(out.get_image()[:, :, ::-1])

import json

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Step 2: Define the path to your JSON file
json_file_path = '/content/drive/MyDrive/dataset/train/ann/00abd8a7-ecd6fc56.jpg.json'

# Step 3: Open and read the JSON file
with open(json_file_path, 'r') as file:
    data = json.load(file)

# Step 4: Print the contents of the JSON file
print(json.dumps(data, indent=4))

"""#Find out the coordinate of the boxes around the Object"""

import json
import cv2
import matplotlib.pyplot as plt
from google.colab import drive
import numpy as np

# Mount Google Drive
drive.mount('/content/drive')

# Define paths
json_file_path = '/content/drive/MyDrive/dataset/train/ann/00abd8a7-ecd6fc56.jpg.json'
image_file_path = '/content/drive/MyDrive/dataset/train/img/00abd8a7-ecd6fc56.jpg'

# Load JSON annotations
with open(json_file_path, 'r') as file:
    annotations = json.load(file)

# Extract bounding box coordinates
predicted_boxes = []

for obj in annotations['objects']:
    if obj['geometryType'] == 'rectangle':
        points = obj['points']['exterior']
        x1, y1 = points[0]
        x2, y2 = points[1]
        predicted_boxes.append([x1, y1, x2, y2])

# Convert to numpy array
predicted_boxesGT = np.array(predicted_boxes, dtype=np.float32)

# Print the array in a readable format
np.set_printoptions(suppress=True)
print(np.array(predicted_boxesGT))

#Extract Model Predictions


def get_model_predictions(predictions):
    pred_boxes = predictions["instances"].pred_boxes.tensor.cpu().numpy()
    pred_classes = predictions["instances"].pred_classes.cpu().numpy()
    return pred_boxes, pred_classes

pred_boxes, pred_classes = get_model_predictions(outputs)
print("Predicted Boxes:", pred_boxes)
print("Predicted Classes:", pred_classes)

import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(10, 20)
        self.dropout1 = nn.Dropout(p=0.5)
        self.fc2 = nn.Linear(20, 20)
        self.dropout2 = nn.Dropout(p=0.5)
        self.fc3 = nn.Linear(20, 10)
        self.fc4 = nn.Linear(64, 32)
        self.dropout3 = nn.Dropout(p=0.5)
        self.fc5 = nn.Linear(10, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        # x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        # x = self.dropout2(x)
        x = F.relu(self.fc3(x))
        # x = F.relu(self.fc4(x))
        # x = self.dropout3(x)
        return self.fc5(x)

import cv2
import matplotlib.pyplot as plt

# Define the image file path
image_file_path = '/content/drive/MyDrive/dataset/train/img/00abd8a7-ecd6fc56.jpg'




# Load the image
image = cv2.imread(image_file_path)
# Convert the image from BGR (OpenCV default) to RGB (matplotlib default)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)


#Red -> Model(pred_boxes)
#Green -> GT (predicted_boxesGT)



# Draw the bounding boxes on the image
# Red for pred_boxes
for box in pred_boxes:
    x_min, y_min, x_max, y_max = map(int, box)
    cv2.rectangle(image_rgb, (x_min, y_min), (x_max, y_max), color=(255, 0, 0), thickness=2)

# Green for predicted_boxes
for box in predicted_boxesGT:
    x_min, y_min, x_max, y_max = map(int, box)
    cv2.rectangle(image_rgb, (x_min, y_min), (x_max, y_max), color=(0, 255, 0), thickness=2)

# Display the image with bounding boxes
plt.figure(figsize=(10, 10))
plt.imshow(image_rgb)
plt.axis('off')  # Hide the axis
plt.show()

# compare the boxes .
# 1. try to find out  if every boxes exist . the one from the model and the one from the dataset
# 2. Compare the position of the boxes
# 3. Compare the label

print(len(pred_boxes))

print(len(predicted_boxesGT))

"""#Find out the probabilities of the detection of the object for each object"""

#Find out the probabilities of the detection of the object for each object
# for example
#predicted_classes = np.array([
#    [0.9, 0.1, 0.0, 0.0],  # car 1
#    [0.1, 0.9, 0.0, 0.0],  # car 2
#    [0.0, 0.0, 0.9, 0.1],  # person
#    [0.0, 0.0, 0.1, 0.9]   # trafic sign
#])
# here the probabilities that the object is a car is 90% , [0.9, 0.1, 0.0, 0.0],

"""#Find out the coordinate of the label for each boxes, we will take an example from the dataset"""

#example of the image is :
#json_file_path = '/content/drive/MyDrive/dataset/train/ann/00abd8a7-ecd6fc56.jpg.json'
#image_file_path = '/content/drive/MyDrive/dataset/train/img/00abd8a7-ecd6fc56.jpg'

!pip install opencv-python-headless matplotlib

import json
import cv2
import matplotlib.pyplot as plt
from google.colab import drive
import numpy as np

# Mount Google Drive
drive.mount('/content/drive')

# Define paths
json_file_path = '/content/drive/MyDrive/dataset/train/ann/00abd8a7-ecd6fc56.jpg.json'
image_file_path = '/content/drive/MyDrive/dataset/train/img/00abd8a7-ecd6fc56.jpg'

# Load JSON annotations
with open(json_file_path, 'r') as file:
    annotations = json.load(file)

# Load image
image = cv2.imread(image_file_path)
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for matplotlib

# Display the image without annotations
plt.figure(figsize=(10, 10))
plt.imshow(image)
plt.axis('off')
plt.show()

def draw_annotations(image, annotations):
    for obj in annotations['objects']:
        points = obj['points']['exterior']
        if obj['geometryType'] == 'rectangle':
            top_left = tuple(points[0])
            bottom_right = tuple(points[1])
            cv2.rectangle(image, top_left, bottom_right, (0, 255, 0), 2)
            cv2.putText(image, obj['classTitle'], top_left, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
    return image

# Draw annotations on the image
annotated_image = draw_annotations(image.copy(), annotations)

# Display the image with annotations
plt.figure(figsize=(12, 8))
plt.imshow(annotated_image)
plt.axis('off')
plt.show()

#get now the coordinate of the boxes

import json
import cv2
import matplotlib.pyplot as plt
from google.colab import drive
import numpy as np

# Mount Google Drive
drive.mount('/content/drive')

# Define paths
json_file_path = '/content/drive/MyDrive/dataset/train/ann/00abd8a7-ecd6fc56.jpg.json'
image_file_path = '/content/drive/MyDrive/dataset/train/img/00abd8a7-ecd6fc56.jpg'

# Load JSON annotations
with open(json_file_path, 'r') as file:
    annotations = json.load(file)

# Extract bounding box coordinates
predicted_boxes = []

for obj in annotations['objects']:
    if obj['geometryType'] == 'rectangle':
        points = obj['points']['exterior']
        x1, y1 = points[0]
        x2, y2 = points[1]
        predicted_boxes.append([x1, y1, x2, y2])

# Convert to numpy array
predicted_boxes = np.array(predicted_boxes, dtype=np.float32)

# Print the array in a readable format
np.set_printoptions(suppress=True)
print(np.array(predicted_boxes))





#Map of the object

"""#Test with two boxes of the image to find problem with the GT"""

import torch
import numpy as np

def distance(box1, box2):
    x1_gt, y1_gt = box1[0], box1[1]
    x1_pred, y1_pred = box2[0], box2[1]
    return np.sqrt((x1_gt - x1_pred)**2 + (y1_gt - y1_pred)**2)

def group_similar_boxes(gt_boxes, pred_boxes, tolerance=4.0):
    """ Group similar Boxes """
    grouped_boxes = []
    used_pred = [False] * len(pred_boxes)
    used_gt = [False] * len(gt_boxes)

    for gt_idx, gt_box in enumerate(gt_boxes):
        current_group = {'gt_box': gt_box, 'pred_boxes': []}

        for pred_idx, pred_box in enumerate(pred_boxes):
            if not used_pred[pred_idx]:
                if distance(gt_box[:2], pred_box[:2]) <= tolerance:
                    current_group['pred_boxes'].append(pred_box)
                    used_pred[pred_idx] = True

        if len(current_group['pred_boxes']) > 0:
            grouped_boxes.append(current_group)
            used_gt[gt_idx] = True

    unmatched_gt_indices = [idx for idx, used in enumerate(used_gt) if not used]
    unmatched_gt_boxes = gt_boxes[unmatched_gt_indices]

    unused_pred_boxes = [pred_boxes[idx] for idx, used in enumerate(used_pred) if not used]

    return grouped_boxes, unmatched_gt_boxes, unused_pred_boxes

gt_boxes = torch.tensor([
    [490., 202., 509., 225.], [475., 197., 494., 248.], [433., 193., 478., 256.],
    [408., 195., 467., 265.], [367., 197., 429., 279.], [197., 167., 383., 310.],
    [613., 211., 640., 233.], [572., 190., 612., 235.], [640., 184., 687., 240.],
    [672., 197., 745., 258.], [508., 195., 592., 276.], [1., 113., 229., 391.],
    [809., 203., 828., 257.], [835., 212., 851., 255.], [839., 212., 854., 252.],
    [907., 214., 922., 271.], [922., 204., 939., 266.], [942., 203., 957., 262.],
    [972., 197., 1002., 262.], [1069., 196., 1106., 258.], [1007., 200., 1047., 284.],
    [1060., 206., 1115., 323.], [1061., 254., 1115., 318.], [930., 211., 982., 309.],
    [589., 163., 602., 180.], [499., 171., 507., 189.], [388., 112., 408., 153.],
    [478., 175., 486., 188.], [751., 207., 767., 245.], [740., 214., 755., 250.],
    [818., 214., 838., 272.], [1149., 125., 1168., 143.], [1187., 208., 1277., 260.]
])

pred_boxes = torch.tensor([
    [373.99905, 206.00848, 428.4977, 276.8802], [1069.3988, 202.05403, 1112.7374, 318.22272],
    [416.27408, 207.055, 467.761, 260.4558], [933.69214, 209.20721, 979.5074, 304.14825],
    [1010.102, 205.39, 1041.7201, 281.76938], [204.49957, 168.20744, 383.337, 310.0184],
    [820.5206, 212.01778, 840.22723, 267.6648], [1188.6769, 211.13678, 1276.0109, 257.51935],
    [436.16266, 198.08429, 488.3349, 252.13927], [638.35876, 189.40385, 687.5557, 242.66507],
    [618.0632, 212.80402, 640.7384, 233.47964], [510.40903, 199.10974, 589.549, 272.01965],
    [674.6412, 195.97755, 743.35956, 258.3464], [841.395, 213.82324, 855.42096, 252.72043],
    [974.94543, 202.25812, 1001.5522, 257.76343], [0.98541236, 135.79276, 226.89935, 386.24506],
    [491.20374, 209.44238, 508.49487, 225.14955], [0., 402.00455, 387.18573, 713.52374],
    [813.9816, 206.87767, 827.7022, 254.59396], [505.35394, 207.30273, 519.9861, 224.73607],
    [923.99054, 211.2875, 941.2391, 266.04877], [592.96295, 165.85947, 600.62134, 179.30354],
    [798.321, 242.43433, 816.57904, 262.4211], [578.7409, 196.66968, 612.9921, 238.4201],
    [461.31226, 198.97017, 494.04626, 249.21362], [968.81226, 231.25874, 1005.1933, 300.01575],
    [578.506, 194.2805, 615.7452, 238.92693]
])

gt_boxes_np = gt_boxes.cpu().numpy()
pred_boxes_np = pred_boxes.cpu().numpy()

sorted_gt_indices = np.argsort(gt_boxes_np[:, 0])
sorted_pred_indices = np.argsort(pred_boxes_np[:, 0])

sorted_gt_boxes = gt_boxes_np[sorted_gt_indices]
sorted_pred_boxes = pred_boxes_np[sorted_pred_indices]

grouped_boxes, unmatched_gt_boxes, unused_pred_boxes = group_similar_boxes(sorted_gt_boxes, sorted_pred_boxes, tolerance=10.0)

print("similar boxes :")
for group in grouped_boxes:
    print("gt_box :", [f"{val:.2f}" for val in group['gt_box']])
    print("pred_boxes :")
    for pred_box in group['pred_boxes']:
        print([f"{val:.2f}" for val in pred_box])
    print("-----------------------")

print("\nGround truth Boxes without Neigbors :")
for box in unmatched_gt_boxes:
    print("gt_box :", [f"{val:.2f}" for val in box])

print("\n unsed boxes of the Model:")
for box in unused_pred_boxes:
    print("pred_box :", [f"{val:.2f}" for val in box])

"""#plot unused boxes from the GT and the dataset"""

from google.colab.patches import cv2_imshow
import cv2
import numpy as np

# Chemin vers l'image
image_file_path = '/content/drive/MyDrive/dataset/train/img/00abd8a7-ecd6fc56.jpg'

# Charger l'image avec OpenCV
image = cv2.imread(image_file_path)

# Définir la couleur des rectangles
color_gt = (0, 255, 0)   # vert pour les boîtes de vérité terrain non appariées
color_pred = (0, 0, 255) # rouge pour les boîtes prédites non utilisées

# Dessiner les rectangles des boîtes de vérité terrain non appariées
for box in unmatched_gt_boxes:
    x1, y1, x2, y2 = box.astype(int)
    cv2.rectangle(image, (x1, y1), (x2, y2), color_gt, 2)  # dessiner un rectangle

# Dessiner les rectangles des boîtes prédites non utilisées
for box in unused_pred_boxes:
    x1, y1, x2, y2 = box.astype(int)
    cv2.rectangle(image, (x1, y1), (x2, y2), color_pred, 2)  # dessiner un rectangle

# Afficher l'image avec les rectangles dessinés
cv2_imshow(image)

#####################
#####################
#####################
#####################
############### Regarding the image we can conclude that there is no error with the Ground truth #####################
############### Because the green box that represent the boxes of the GT are correct #####################
#####################
#####################
#####################
#####################

"""#go throw the image of /train/img to find out if there is a problem with the GT"""

import cv2
import json
import os
import numpy as np
import matplotlib.pyplot as plt

# Chemin vers le dossier d'images
# Chemins vers les répertoires d'images et d'annotations
image_dir = '/content/drive/MyDrive/dataset/train/img/'
annotation_dir = '/content/drive/MyDrive/dataset/train/ann/'

# Liste des fichiers d'images
image_files = os.listdir(image_dir)

def draw_annotations(image, annotations):
    for obj in annotations['objects']:
        points = obj['points']['exterior']
        if obj['geometryType'] == 'rectangle':
            top_left = tuple(points[0])
            bottom_right = tuple(points[1])
            cv2.rectangle(image, top_left, bottom_right, (0, 255, 0), 2)
            # Convert top_left to string for putText
            text = ""
            cv2.putText(image, text, top_left, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
    return image


def get_model_predictions(predictions):
    pred_boxes = predictions["instances"].pred_boxes.tensor.cpu().numpy()
    pred_classes = predictions["instances"].pred_classes.cpu().numpy()
    return pred_boxes, pred_classes


for image_file in image_files:
    if image_file.endswith('.jpg'):  # Assurez-vous que le fichier est une image JPG
        image_path = os.path.join(image_dir, image_file)
        annotation_path = os.path.join(annotation_dir, image_file.replace('.jpg', '.jpg.json'))

        im = cv2.imread(image_path)
        image = cv2.imread(image_path)

        with open(annotation_path, 'r') as file:
            annotations = json.load(file)
        # Initialize the predictor
        predictor = DefaultPredictor(cfg)


        # Perform object detection
        outputs = predictor(image)
        pred_boxes, pred_classes = get_model_predictions(outputs)

        for box in pred_boxes:
          x_min, y_min, x_max, y_max = map(int, box)
          cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color=(255, 0, 0), thickness=2)


       #perform object detection with detectron to and plot the boxes on the image with a color red


        # Draw annotations on the image
        annotated_image = draw_annotations(image.copy(), annotations)

        # Display the image with annotations
        plt.figure(figsize=(12, 8))
        plt.imshow(annotated_image)
        plt.axis('off')
        plt.show()

"""#object detected by the model not present in the GT"""